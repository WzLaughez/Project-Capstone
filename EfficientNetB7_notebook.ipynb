{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9203603,"sourceType":"datasetVersion","datasetId":5443371}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORT DATASET","metadata":{"id":"_4BBhXAGQKGB"}},{"cell_type":"code","source":"\"\"\"# Install kaggle dan kagglehub jika belum\n!pip install -q kagglehub\n\nimport kagglehub\n\n# Download dataset dari Kaggle (otomatis simpan ke cache)\npath = kagglehub.dataset_download(\"aliefrahmanhakim/type-of-plastic-waste-dataset\")\nprint(\"Path to dataset files:\", path)\"\"\"","metadata":{"id":"mlKwtiSuhWJ6","outputId":"55c7d020-7c61-4a25-c037-da9b373b128e","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:14:49.212860Z","iopub.execute_input":"2025-06-08T10:14:49.213167Z","iopub.status.idle":"2025-06-08T10:14:49.226551Z","shell.execute_reply.started":"2025-06-08T10:14:49.213137Z","shell.execute_reply":"2025-06-08T10:14:49.225462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport shutil\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, Sequential, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import (GlobalAveragePooling2D, Conv2D, MaxPool2D, Flatten, Dense,\n                                     Dropout, Input, BatchNormalization)\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Mengabaikan peringatan\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"id":"Sk2BZvd5vhkr","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:14:49.228293Z","iopub.execute_input":"2025-06-08T10:14:49.228706Z","iopub.status.idle":"2025-06-08T10:15:11.721411Z","shell.execute_reply.started":"2025-06-08T10:14:49.228635Z","shell.execute_reply":"2025-06-08T10:15:11.720459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA LOAD","metadata":{"id":"6Fot6uGBQE-U"}},{"cell_type":"code","source":"# Path utama dataset\ndf = \"/kaggle/input/type-of-plastic-waste-dataset\"\n\n# 1. Kumpulkan semua gambar dan kelompokkan berdasarkan label\nlabel_to_images = {}  # Kamus: {label: [list_path_gambar]}\n\nfor root, dirs, files in os.walk(df):\n    for file in files:\n        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            label = os.path.basename(root)  # Ambil label dari nama folder\n            if label not in label_to_images:\n                label_to_images[label] = []\n            label_to_images[label].append(os.path.join(root, file))\n\n# 2. Pilih 1 gambar acak per label (maksimal 4 label)\nlabels = list(label_to_images.keys())\nif len(labels) > 4:\n    labels = random.sample(labels, 4)  # Ambil 4 label acak jika label > 4\n\nselected_images = []\nfor label in labels:\n    images = label_to_images[label]\n    selected_images.append((label, random.choice(images)))  # (label, path_gambar)\n\n# 3. Tampilkan gambar\nplt.figure(figsize=(15, 5))\nplt.suptitle(\"Contoh Gambar per Label\", fontsize=16, y=1.05)  # Judul utama\n\nfor i, (label, img_path) in enumerate(selected_images):\n    img = Image.open(img_path)\n    \n    plt.subplot(1, 4, i + 1)\n    plt.imshow(img)\n    plt.title(f\"Label: {label}\", pad=10)  # `pad` memberi jarak antara judul dan gambar\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:15:11.722479Z","iopub.execute_input":"2025-06-08T10:15:11.723924Z","iopub.status.idle":"2025-06-08T10:15:32.276747Z","shell.execute_reply.started":"2025-06-08T10:15:11.723886Z","shell.execute_reply":"2025-06-08T10:15:32.275712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA UNDERSTANDING","metadata":{"id":"NQuMuUtaQXYj"}},{"cell_type":"code","source":"# Menghitung jumlah gambar per kategori\ncategory_counts = {}\ntotal_images = 0  \nimage_paths = []\n\nfor root, dirs, files in os.walk(df):\n    for file in files:\n        if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n            label = os.path.basename(root)  \n            category_counts[label] = category_counts.get(label, 0) + 1\n            total_images += 1\n            image_paths.append(os.path.join(root, file))\n\n# Konversi ke DataFrame\ndf_counts = pd.DataFrame(list(category_counts.items()), columns=[\"Label\", \"Jumlah Gambar\"])\ndf_counts = df_counts.sort_values(\"Jumlah Gambar\", ascending=False)  \n\n# Visualisasi bar chart\nplt.figure(figsize=(10, 6))\nbarplot = sns.barplot(\n    data=df_counts,\n    y=\"Label\",  \n    x=\"Jumlah Gambar\",  \n    palette=\"viridis\",  \n    orient=\"h\"  \n)\n\n# Tambahkan nilai di ujung bar\nfor i, value in enumerate(df_counts[\"Jumlah Gambar\"]):\n    barplot.text(\n        value + max(df_counts[\"Jumlah Gambar\"])*0.01,  \n        i,  # Posisi Y\n        f\"{value:,}\", \n        ha='left',  \n        va='center',  \n        fontsize=10\n    )\n\nplt.title(f\"Distribusi Jumlah Gambar per Kategori\\nTotal Dataset: {total_images} gambar\", pad=20)\nplt.xlabel(\"Jumlah Gambar\", labelpad=10)\nplt.ylabel(\"Kategori Plastik\", labelpad=10)\nplt.grid(axis='x', linestyle='--', alpha=0.4)  \nsns.despine(left=True)  \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:15:32.277988Z","iopub.execute_input":"2025-06-08T10:15:32.278273Z","iopub.status.idle":"2025-06-08T10:15:38.636481Z","shell.execute_reply.started":"2025-06-08T10:15:32.278250Z","shell.execute_reply":"2025-06-08T10:15:38.635543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Menampilkan distribusi ukuran gambar\nimage_sizes = []\nsample_images = random.sample(image_paths, min(100, len(image_paths)))  # contoh 100 gambar\n\nfor img_path in sample_images:\n    try:\n        with Image.open(img_path) as img:\n            image_sizes.append(img.size)  # (width, height)\n    except:\n        continue\n\n# Konversi ke DataFrame\ndf_sizes = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\n\n# Visualisasi distribusi ukuran\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(df_sizes['Width'], kde=True, color='skyblue')\nplt.title(\"Distribusi Lebar Gambar\")\n\nplt.subplot(1, 2, 2)\nsns.histplot(df_sizes['Height'], kde=True, color='salmon')\nplt.title(\"Distribusi Tinggi Gambar\")\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"3FtKwlBBnxXD","outputId":"b8af9038-ac7c-43c9-ab2b-a8724254d316","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:15:38.638701Z","iopub.execute_input":"2025-06-08T10:15:38.639022Z","iopub.status.idle":"2025-06-08T10:15:40.196956Z","shell.execute_reply.started":"2025-06-08T10:15:38.639000Z","shell.execute_reply":"2025-06-08T10:15:40.196091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA SPLITTING","metadata":{"id":"deedaeSyQdIl"}},{"cell_type":"code","source":"# Direktori training dan testing (80/20)\nTRAIN_DIR = \"/kaggle/input/type-of-plastic-waste-dataset/train\"\nTEST_DIR = \"/kaggle/input/type-of-plastic-waste-dataset/val\"\n\ntrain_HDPE = os.path.join(TRAIN_DIR + '/HDPE')\ntrain_PET = os.path.join(TRAIN_DIR + '/PET')\ntrain_PP = os.path.join(TRAIN_DIR + '/PP')\ntrain_PS = os.path.join(TRAIN_DIR + '/PS')\ntest_HDPE = os.path.join(TEST_DIR + '/HDPE')\ntest_PET = os.path.join(TEST_DIR + '/PET')\ntest_PP = os.path.join(TEST_DIR + '/PP')\ntest_PS = os.path.join(TEST_DIR + '/PS')\n\nprint(\"TRAIN\\n\")\nprint(\"Total number of HDPE images in training set: \",len(os.listdir(train_HDPE)))\nprint(\"Total number of PET images in training set: \",len(os.listdir(train_PET)))\nprint(\"Total number of PP images in training set: \",len(os.listdir(train_PP)))\nprint(\"Total number of PS images in training set: \",len(os.listdir(train_PS)))\nprint(\"\\nTEST\\n\")\nprint(\"Total number of HDPE images in test set: \",len(os.listdir(test_HDPE)))\nprint(\"Total number of PET images in test set: \",len(os.listdir(test_PET)))\nprint(\"Total number of PP images in test set: \",len(os.listdir(test_PP)))\nprint(\"Total number of PS images in test set: \",len(os.listdir(test_PS)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:15:40.267603Z","iopub.execute_input":"2025-06-08T10:15:40.267874Z","iopub.status.idle":"2025-06-08T10:15:40.309058Z","shell.execute_reply.started":"2025-06-08T10:15:40.267853Z","shell.execute_reply":"2025-06-08T10:15:40.307904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mempersiapkan data gambar untuk pelatihan\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  \n    validation_split=0.2\n)\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input   \n)\n\ntrain_generator = datagen.flow_from_directory(TRAIN_DIR,\n                                              batch_size=32,\n                                              target_size=(512,512),\n                                              class_mode='categorical',\n                                              subset='training',\n                                              shuffle=True)\n\nvalidation_generator = datagen.flow_from_directory(TRAIN_DIR,\n                                                   batch_size=32,\n                                                   target_size=(512,512),\n                                                   class_mode='categorical',\n                                                   subset='validation',\n                                                   shuffle=False)\n\ntest_generator = test_datagen.flow_from_directory(TEST_DIR,\n                                                  batch_size=1,\n                                                  target_size=(512,512),\n                                                  class_mode='categorical',\n                                                  shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:22:21.128012Z","iopub.execute_input":"2025-06-07T16:22:21.128240Z","iopub.status.idle":"2025-06-07T16:22:23.030564Z","shell.execute_reply.started":"2025-06-07T16:22:21.128221Z","shell.execute_reply":"2025-06-07T16:22:23.029534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODELLING","metadata":{"id":"ksRlSXuOQhek"}},{"cell_type":"markdown","source":"## EfficientNet","metadata":{}},{"cell_type":"code","source":"# Load model EfficientNetB7\nbase_model = EfficientNetB7(\n    weights='imagenet', \n    include_top=False, \n    input_shape=(512,512,3)  \n)\n\n# Menambahkan top model \nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = BatchNormalization()(x)\nx = Dense(512, activation='relu')(x)  \nx = Dropout(0.5)(x)  # Dropout \nx = Dense(256, activation='relu')(x)  # Layer tambahan\nx = Dropout(0.3)(x) \npredictions = Dense(4, activation='softmax')(x) \n\nmodel_3 = Model(inputs=base_model.input, outputs=predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:26:32.396387Z","iopub.execute_input":"2025-06-07T16:26:32.397366Z","iopub.status.idle":"2025-06-07T16:26:38.161981Z","shell.execute_reply.started":"2025-06-07T16:26:32.397331Z","shell.execute_reply":"2025-06-07T16:26:38.161155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RESULT_PATH = ''  # Ganti dengan path hasil pelatihan\nOUTPUT_DIR = '/kaggle/working/outputs'\n\nif os.path.exists(f\"{RESULT_PATH}/training_outputs\"):\n    print(\"Memuat file hasil training sebelumnya...\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Copy dari dataset ke working directory\n    shutil.copytree(f\"{RESULT_PATH}/training_outputs\", OUTPUT_DIR, dirs_exist_ok=True)\n    \n    # Pindahkan file ke root working directory\n    for file in ['final_model_3.h5', 'best_model_3.h5', 'training_history_3.csv', 'training_log_3.csv']:\n        if os.path.exists(f\"{OUTPUT_DIR}/{file}\"):\n            shutil.copy(f\"{OUTPUT_DIR}/{file}\", '/kaggle/working/')\n\n\nmodel_path = 'final_model_3.h5'\nbest_model_path = 'best_model_3.h5'\nhistory_path = 'training_history_3.csv'\nlog_path = 'training_log_3.csv'\nretrain = False # Ganti menjadi true jika ingin training ulang\n\nif os.path.exists(model_path) and not retrain:\n    print(\"Model sudah dilatih sebelumnya. Memuat hasil yang tersimpan...\")\n    \n    # Muat model\n    model_3 = load_model(model_path)\n    \n    # Muat history dan tampilkan\n    history_df = pd.read_csv(history_path)\n    print(\"\\n=== Riwayat Pelatihan ===\")\n    print(history_df.tail())  \n    \n    # Tampilkan log epoch dari CSVLogger \n    if os.path.exists(log_path):\n        epoch_logs = pd.read_csv(log_path)\n        print(\"\\n=== Log Epoch Lengkap ===\")\n        print(epoch_logs)\n\nelse:\n    print(\"Model belum dilatih. Memulai pelatihan...\")\n    \n    # Hapus file lama jika ada\n    for f in [model_path, history_path, log_path, best_model_path]:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    # Simpan log training ke CSV\n    csv_logger = CSVLogger(log_path)\n    \n    # Simpan model terbaik selama training\n    checkpoint = ModelCheckpoint(\n        best_model_path,\n        monitor='val_loss',\n        save_best_only=True,\n        mode='min',\n        verbose=1\n    )\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=15,  \n        min_delta=0.0001,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    reduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=5,\n    min_lr=1e-7,\n    mode='min',\n    verbose=1\n    )\n    \n    class_indices = train_generator.classes\n    class_weights = compute_class_weight('balanced', \n                                        classes=np.unique(class_indices), \n                                        y=class_indices)\n    class_weights_dict = dict(enumerate(class_weights))\n        \n    base_model.trainable = False\n\n    # Melatih Head model\n    initial_lr = 1e-3 \n    model_3.compile(\n        optimizer=Adam(learning_rate=initial_lr),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Melatih head epoch\n    epochs_head = 15\n    history_head = model_3.fit(\n        train_generator,\n        epochs=epochs_head, # Latih head selama 10 epoch dulu\n        validation_data=validation_generator,\n        class_weight=class_weights_dict,\n        callbacks=[csv_logger, checkpoint, early_stopping, reduce_lr] # Tanpa unfreeze callback\n    )\n\n    # Unfreeze (fine-tuning)\n    base_model.trainable = True\n    \n    freeze_first_n = 400\n    for layer in base_model.layers[:freeze_first_n]:\n        layer.trainable = False\n    \n    finetune_lr = 1e-5\n    model_3.compile(\n        optimizer=Adam(learning_rate=finetune_lr), # LR rendah!\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Melanjutkan pelatihan untuk sisa epoch\n    total_epochs = 100\n    initial_epoch_finetune = history_head.epoch[-1] + 1\n    \n    history_finetune = model_3.fit(\n        train_generator,\n        epochs=total_epochs, \n        initial_epoch=initial_epoch_finetune, \n        validation_data=validation_generator,\n        class_weight=class_weights_dict,\n        callbacks=[csv_logger, checkpoint, early_stopping, reduce_lr] \n    )\n    print(f\"Memuat bobot terbaik dari {best_model_path} sebelum menyimpan model final.\")\n    model_3.load_weights(best_model_path)\n    \n    # Simpan model akhir\n    model_3.save(model_path)\n    \n    # Simpan history ke DataFrame\n    history_head_df = pd.DataFrame(history_head.history)\n    history_finetune_df = pd.DataFrame(history_finetune.history)\n    full_history_df = pd.concat([history_head_df, history_finetune_df], ignore_index=True)\n    full_history_df.to_csv(history_path, index=False)\n\n    print(f\"Model akhir disimpan di: {model_path}\")\n    print(f\"History lengkap disimpan di: {history_path}\")\n    \n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    for file in [model_path, best_model_path, history_path, log_path]:\n        if os.path.exists(file):\n            shutil.copy(file, OUTPUT_DIR)\n    \n    shutil.make_archive('training_outputs', 'zip', OUTPUT_DIR)\n    print(\"File hasil training siap di-download!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:26:38.163239Z","iopub.execute_input":"2025-06-07T16:26:38.163551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Menampilkan grafik hasil akurasi dan loss \nacc = full_history_df['accuracy']\nval_acc = full_history_df['val_accuracy']\nloss = full_history_df['loss']\nval_loss = full_history_df['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.title('Training and Validation Loss')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Menampilkan confusion matrix\ntest_generator.reset()\n\npreds_3 = model_3.predict(test_generator, steps=len(test_generator), verbose=1)\npred_labels = np.argmax(preds_3, axis=1)  \n\n# Ground truth\ntrue_labels = test_generator.classes\nclass_names = list(test_generator.class_indices.keys())  \n\n# Confusion Matrix\ncm = confusion_matrix(true_labels, pred_labels)\ncm_df = pd.DataFrame(cm, index=[f'Actual {c}' for c in class_names],\n                        columns=[f'Predicted {c}' for c in class_names])\n\n# Plot Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Actual Class\")\nplt.xlabel(\"Predicted Class\")\nplt.show()\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels, pred_labels, target_names=class_names, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}